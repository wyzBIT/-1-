#### 不同的learning rate

**Training Configuration:**

**Learning Rate: 0.01**

**Batch Size: 64**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.3199

Epoch 1, Batch 200, Loss: 2.0999

Epoch 1, Batch 300, Loss: 1.9910

Epoch 1, Batch 400, Loss: 1.8932

Epoch 1, Batch 500, Loss: 1.8438

Epoch 1, Batch 600, Loss: 1.7976

Epoch 1, Batch 700, Loss: 1.7575

Epoch 2, Batch 100, Loss: 1.7485

Epoch 2, Batch 200, Loss: 1.7326

Epoch 2, Batch 300, Loss: 1.7026

Epoch 2, Batch 400, Loss: 1.7011

Epoch 2, Batch 500, Loss: 1.7010

Epoch 2, Batch 600, Loss: 1.7229

Epoch 2, Batch 700, Loss: 1.7053

Epoch 3, Batch 100, Loss: 1.6808

Epoch 3, Batch 200, Loss: 1.6666

Epoch 3, Batch 300, Loss: 1.6632

Epoch 3, Batch 400, Loss: 1.6717

Epoch 3, Batch 500, Loss: 1.6870

Epoch 3, Batch 600, Loss: 1.6785

Epoch 3, Batch 700, Loss: 1.6434

Epoch 4, Batch 100, Loss: 1.6687

Epoch 4, Batch 200, Loss: 1.6848

Epoch 4, Batch 300, Loss: 1.6454

Epoch 4, Batch 400, Loss: 1.6493

Epoch 4, Batch 500, Loss: 1.6378

Epoch 4, Batch 600, Loss: 1.6228

Epoch 4, Batch 700, Loss: 1.6524

Epoch 5, Batch 100, Loss: 1.6577

Epoch 5, Batch 200, Loss: 1.6518

Epoch 5, Batch 300, Loss: 1.6528

Epoch 5, Batch 400, Loss: 1.6241

Epoch 5, Batch 500, Loss: 1.6360

Epoch 5, Batch 600, Loss: 1.6584

Epoch 5, Batch 700, Loss: 1.6272

Epoch 6, Batch 100, Loss: 1.6346

Epoch 6, Batch 200, Loss: 1.6444

Epoch 6, Batch 300, Loss: 1.6231

Epoch 6, Batch 400, Loss: 1.6163

Epoch 6, Batch 500, Loss: 1.5846

Epoch 6, Batch 600, Loss: 1.6162

Epoch 6, Batch 700, Loss: 1.6543

Epoch 7, Batch 100, Loss: 1.6032

Epoch 7, Batch 200, Loss: 1.6212

Epoch 7, Batch 300, Loss: 1.6124

Epoch 7, Batch 400, Loss: 1.6129

Epoch 7, Batch 500, Loss: 1.6118

Epoch 7, Batch 600, Loss: 1.5970

Epoch 7, Batch 700, Loss: 1.6253

Epoch 8, Batch 100, Loss: 1.6097

Epoch 8, Batch 200, Loss: 1.6180

Epoch 8, Batch 300, Loss: 1.5977

Epoch 8, Batch 400, Loss: 1.6329

Epoch 8, Batch 500, Loss: 1.6204

Epoch 8, Batch 600, Loss: 1.5986

Epoch 8, Batch 700, Loss: 1.5887

Epoch 9, Batch 100, Loss: 1.5939

Epoch 9, Batch 200, Loss: 1.5789

Epoch 9, Batch 300, Loss: 1.5823

Epoch 9, Batch 400, Loss: 1.5850

Epoch 9, Batch 500, Loss: 1.6337

Epoch 9, Batch 600, Loss: 1.6084

Epoch 9, Batch 700, Loss: 1.6038

Epoch 10, Batch 100, Loss: 1.5610

Epoch 10, Batch 200, Loss: 1.5950

Epoch 10, Batch 300, Loss: 1.5797

Epoch 10, Batch 400, Loss: 1.6014

Epoch 10, Batch 500, Loss: 1.6007

Epoch 10, Batch 600, Loss: 1.5576

Epoch 10, Batch 700, Loss: 1.6058

Finished Training

Accuracy on test set: 45.28%

**Training Configuration:**

**Learning Rate: 0.001**

**Batch Size: 64**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.1321

Epoch 1, Batch 200, Loss: 1.7944

Epoch 1, Batch 300, Loss: 1.6513

Epoch 1, Batch 400, Loss: 1.5524

Epoch 1, Batch 500, Loss: 1.4982

Epoch 1, Batch 600, Loss: 1.4579

Epoch 1, Batch 700, Loss: 1.4071

Epoch 2, Batch 100, Loss: 1.3324

Epoch 2, Batch 200, Loss: 1.2882

Epoch 2, Batch 300, Loss: 1.2804

Epoch 2, Batch 400, Loss: 1.2460

Epoch 2, Batch 500, Loss: 1.2193

Epoch 2, Batch 600, Loss: 1.1867

Epoch 2, Batch 700, Loss: 1.1841

Epoch 3, Batch 100, Loss: 1.1327

Epoch 3, Batch 200, Loss: 1.0882

Epoch 3, Batch 300, Loss: 1.0904

Epoch 3, Batch 400, Loss: 1.0633

Epoch 3, Batch 500, Loss: 1.0585

Epoch 3, Batch 600, Loss: 1.0629

Epoch 3, Batch 700, Loss: 1.0411

Epoch 4, Batch 100, Loss: 0.9647

Epoch 4, Batch 200, Loss: 0.9738

Epoch 4, Batch 300, Loss: 0.9840

Epoch 4, Batch 400, Loss: 0.9645

Epoch 4, Batch 500, Loss: 0.9614

Epoch 4, Batch 600, Loss: 0.9530

Epoch 4, Batch 700, Loss: 0.9282

Epoch 5, Batch 100, Loss: 0.8825

Epoch 5, Batch 200, Loss: 0.8793

Epoch 5, Batch 300, Loss: 0.8818

Epoch 5, Batch 400, Loss: 0.8790

Epoch 5, Batch 500, Loss: 0.8614

Epoch 5, Batch 600, Loss: 0.8674

Epoch 5, Batch 700, Loss: 0.8232

Epoch 6, Batch 100, Loss: 0.8048

Epoch 6, Batch 200, Loss: 0.7863

Epoch 6, Batch 300, Loss: 0.8049

Epoch 6, Batch 400, Loss: 0.8156

Epoch 6, Batch 500, Loss: 0.8318

Epoch 6, Batch 600, Loss: 0.7909

Epoch 6, Batch 700, Loss: 0.8031

Epoch 7, Batch 100, Loss: 0.7081

Epoch 7, Batch 200, Loss: 0.7252

Epoch 7, Batch 300, Loss: 0.7523

Epoch 7, Batch 400, Loss: 0.7511

Epoch 7, Batch 500, Loss: 0.7651

Epoch 7, Batch 600, Loss: 0.7628

Epoch 7, Batch 700, Loss: 0.7487

Epoch 8, Batch 100, Loss: 0.6659

Epoch 8, Batch 200, Loss: 0.7021

Epoch 8, Batch 300, Loss: 0.6947

Epoch 8, Batch 400, Loss: 0.7053

Epoch 8, Batch 500, Loss: 0.7200

Epoch 8, Batch 600, Loss: 0.7072

Epoch 8, Batch 700, Loss: 0.7121

Epoch 9, Batch 100, Loss: 0.6555

Epoch 9, Batch 200, Loss: 0.6683

Epoch 9, Batch 300, Loss: 0.6432

Epoch 9, Batch 400, Loss: 0.6672

Epoch 9, Batch 500, Loss: 0.6732

Epoch 9, Batch 600, Loss: 0.6603

Epoch 9, Batch 700, Loss: 0.6662

Epoch 10, Batch 100, Loss: 0.5849

Epoch 10, Batch 200, Loss: 0.6083

Epoch 10, Batch 300, Loss: 0.6189

Epoch 10, Batch 400, Loss: 0.6043

Epoch 10, Batch 500, Loss: 0.6427

Epoch 10, Batch 600, Loss: 0.6339

Epoch 10, Batch 700, Loss: 0.6473

Finished Training

Accuracy on test set: 74.55%

**Training Configuration:**

**Learning Rate: 0.0001**

**Batch Size: 64**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.2491

Epoch 1, Batch 200, Loss: 2.0592

Epoch 1, Batch 300, Loss: 1.9493

Epoch 1, Batch 400, Loss: 1.8586

Epoch 1, Batch 500, Loss: 1.8097

Epoch 1, Batch 600, Loss: 1.7689

Epoch 1, Batch 700, Loss: 1.7320

Epoch 2, Batch 100, Loss: 1.6742

Epoch 2, Batch 200, Loss: 1.6415

Epoch 2, Batch 300, Loss: 1.6356

Epoch 2, Batch 400, Loss: 1.5929

Epoch 2, Batch 500, Loss: 1.5950

Epoch 2, Batch 600, Loss: 1.5626

Epoch 2, Batch 700, Loss: 1.5709

Epoch 3, Batch 100, Loss: 1.5142

Epoch 3, Batch 200, Loss: 1.5078

Epoch 3, Batch 300, Loss: 1.5068

Epoch 3, Batch 400, Loss: 1.5100

Epoch 3, Batch 500, Loss: 1.4907

Epoch 3, Batch 600, Loss: 1.4779

Epoch 3, Batch 700, Loss: 1.4655

Epoch 4, Batch 100, Loss: 1.4316

Epoch 4, Batch 200, Loss: 1.4347

Epoch 4, Batch 300, Loss: 1.4271

Epoch 4, Batch 400, Loss: 1.4613

Epoch 4, Batch 500, Loss: 1.3993

Epoch 4, Batch 600, Loss: 1.4254

Epoch 4, Batch 700, Loss: 1.4067

Epoch 5, Batch 100, Loss: 1.3762

Epoch 5, Batch 200, Loss: 1.3845

Epoch 5, Batch 300, Loss: 1.3806

Epoch 5, Batch 400, Loss: 1.3608

Epoch 5, Batch 500, Loss: 1.3651

Epoch 5, Batch 600, Loss: 1.3691

Epoch 5, Batch 700, Loss: 1.3745

Epoch 6, Batch 100, Loss: 1.3746

Epoch 6, Batch 200, Loss: 1.3273

Epoch 6, Batch 300, Loss: 1.3309

Epoch 6, Batch 400, Loss: 1.3132

Epoch 6, Batch 500, Loss: 1.3005

Epoch 6, Batch 600, Loss: 1.3207

Epoch 6, Batch 700, Loss: 1.3359

Epoch 7, Batch 100, Loss: 1.3024

Epoch 7, Batch 200, Loss: 1.3073

Epoch 7, Batch 300, Loss: 1.2923

Epoch 7, Batch 400, Loss: 1.2694

Epoch 7, Batch 500, Loss: 1.2797

Epoch 7, Batch 600, Loss: 1.2940

Epoch 7, Batch 700, Loss: 1.2732

Epoch 8, Batch 100, Loss: 1.2376

Epoch 8, Batch 200, Loss: 1.2431

Epoch 8, Batch 300, Loss: 1.2657

Epoch 8, Batch 400, Loss: 1.2416

Epoch 8, Batch 500, Loss: 1.2680

Epoch 8, Batch 600, Loss: 1.2602

Epoch 8, Batch 700, Loss: 1.2394

Epoch 9, Batch 100, Loss: 1.2214

Epoch 9, Batch 200, Loss: 1.2314

Epoch 9, Batch 300, Loss: 1.2273

Epoch 9, Batch 400, Loss: 1.2389

Epoch 9, Batch 500, Loss: 1.2166

Epoch 9, Batch 600, Loss: 1.2204

Epoch 9, Batch 700, Loss: 1.2094

Epoch 10, Batch 100, Loss: 1.2145

Epoch 10, Batch 200, Loss: 1.1842

Epoch 10, Batch 300, Loss: 1.1808

Epoch 10, Batch 400, Loss: 1.2035

Epoch 10, Batch 500, Loss: 1.1750

Epoch 10, Batch 600, Loss: 1.1759

Epoch 10, Batch 700, Loss: 1.1648

Finished Training

Accuracy on test set: 59.45%

#### 不同的优化器

**Training Configuration:**

**Learning Rate: 0.001**

**Batch Size: 64**

**Optimizer: sgd**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.3029

Epoch 1, Batch 200, Loss: 2.3012

Epoch 1, Batch 300, Loss: 2.3004

Epoch 1, Batch 400, Loss: 2.2997

Epoch 1, Batch 500, Loss: 2.2974

Epoch 1, Batch 600, Loss: 2.2959

Epoch 1, Batch 700, Loss: 2.2917

Epoch 2, Batch 100, Loss: 2.2847

Epoch 2, Batch 200, Loss: 2.2745

Epoch 2, Batch 300, Loss: 2.2554

Epoch 2, Batch 400, Loss: 2.2299

Epoch 2, Batch 500, Loss: 2.1905

Epoch 2, Batch 600, Loss: 2.1560

Epoch 2, Batch 700, Loss: 2.1013

Epoch 3, Batch 100, Loss: 2.0284

Epoch 3, Batch 200, Loss: 2.0346

Epoch 3, Batch 300, Loss: 1.9997

Epoch 3, Batch 400, Loss: 1.9968

Epoch 3, Batch 500, Loss: 1.9839

Epoch 3, Batch 600, Loss: 1.9743

Epoch 3, Batch 700, Loss: 1.9557

Epoch 4, Batch 100, Loss: 1.9165

Epoch 4, Batch 200, Loss: 1.9010

Epoch 4, Batch 300, Loss: 1.8912

Epoch 4, Batch 400, Loss: 1.8852

Epoch 4, Batch 500, Loss: 1.8666

Epoch 4, Batch 600, Loss: 1.8386

Epoch 4, Batch 700, Loss: 1.8156

Epoch 5, Batch 100, Loss: 1.7976

Epoch 5, Batch 200, Loss: 1.7646

Epoch 5, Batch 300, Loss: 1.7491

Epoch 5, Batch 400, Loss: 1.7181

Epoch 5, Batch 500, Loss: 1.7043

Epoch 5, Batch 600, Loss: 1.7102

Epoch 5, Batch 700, Loss: 1.7008

Epoch 6, Batch 100, Loss: 1.6650

Epoch 6, Batch 200, Loss: 1.6462

Epoch 6, Batch 300, Loss: 1.6332

Epoch 6, Batch 400, Loss: 1.6299

Epoch 6, Batch 500, Loss: 1.6190

Epoch 6, Batch 600, Loss: 1.6250

Epoch 6, Batch 700, Loss: 1.6195

Epoch 7, Batch 100, Loss: 1.5984

Epoch 7, Batch 200, Loss: 1.5872

Epoch 7, Batch 300, Loss: 1.5772

Epoch 7, Batch 400, Loss: 1.5590

Epoch 7, Batch 500, Loss: 1.5301

Epoch 7, Batch 600, Loss: 1.5551

Epoch 7, Batch 700, Loss: 1.5337

Epoch 8, Batch 100, Loss: 1.5295

Epoch 8, Batch 200, Loss: 1.5184

Epoch 8, Batch 300, Loss: 1.5250

Epoch 8, Batch 400, Loss: 1.5143

Epoch 8, Batch 500, Loss: 1.4903

Epoch 8, Batch 600, Loss: 1.5070

Epoch 8, Batch 700, Loss: 1.4947

Epoch 9, Batch 100, Loss: 1.4901

Epoch 9, Batch 200, Loss: 1.4796

Epoch 9, Batch 300, Loss: 1.4687

Epoch 9, Batch 400, Loss: 1.4634

Epoch 9, Batch 500, Loss: 1.4761

Epoch 9, Batch 600, Loss: 1.4720

Epoch 9, Batch 700, Loss: 1.4508

Epoch 10, Batch 100, Loss: 1.4398

Epoch 10, Batch 200, Loss: 1.4479

Epoch 10, Batch 300, Loss: 1.4207

Epoch 10, Batch 400, Loss: 1.4349

Epoch 10, Batch 500, Loss: 1.4045

Epoch 10, Batch 600, Loss: 1.3962

Epoch 10, Batch 700, Loss: 1.4172

Finished Training

Accuracy on test set: 51.73%

#### 不同的batch size

**Training Configuration:**

**Learning Rate: 0.001**

**Batch Size: 32**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.1863

Epoch 1, Batch 200, Loss: 1.9960

Epoch 1, Batch 300, Loss: 1.8214

Epoch 1, Batch 400, Loss: 1.7457

Epoch 1, Batch 500, Loss: 1.6604

Epoch 1, Batch 600, Loss: 1.5886

Epoch 1, Batch 700, Loss: 1.5785

Epoch 1, Batch 800, Loss: 1.5242

Epoch 1, Batch 900, Loss: 1.5307

Epoch 1, Batch 1000, Loss: 1.4800

Epoch 1, Batch 1100, Loss: 1.4533

Epoch 1, Batch 1200, Loss: 1.4240

Epoch 1, Batch 1300, Loss: 1.4128

Epoch 1, Batch 1400, Loss: 1.3583

Epoch 1, Batch 1500, Loss: 1.3606

Epoch 2, Batch 100, Loss: 1.3082

Epoch 2, Batch 200, Loss: 1.2725

Epoch 2, Batch 300, Loss: 1.2856

Epoch 2, Batch 400, Loss: 1.2835

Epoch 2, Batch 500, Loss: 1.2925

Epoch 2, Batch 600, Loss: 1.3012

Epoch 2, Batch 700, Loss: 1.2387

Epoch 2, Batch 800, Loss: 1.2390

Epoch 2, Batch 900, Loss: 1.2203

Epoch 2, Batch 1000, Loss: 1.1656

Epoch 2, Batch 1100, Loss: 1.1837

Epoch 2, Batch 1200, Loss: 1.1171

Epoch 2, Batch 1300, Loss: 1.1725

Epoch 2, Batch 1400, Loss: 1.1179

Epoch 2, Batch 1500, Loss: 1.1740

Epoch 3, Batch 100, Loss: 1.1026

Epoch 3, Batch 200, Loss: 1.0703

Epoch 3, Batch 300, Loss: 1.0599

Epoch 3, Batch 400, Loss: 1.0633

Epoch 3, Batch 500, Loss: 1.0768

Epoch 3, Batch 600, Loss: 1.0650

Epoch 3, Batch 700, Loss: 1.0783

Epoch 3, Batch 800, Loss: 1.0334

Epoch 3, Batch 900, Loss: 1.0991

Epoch 3, Batch 1000, Loss: 1.0728

Epoch 3, Batch 1100, Loss: 1.0259

Epoch 3, Batch 1200, Loss: 1.0647

Epoch 3, Batch 1300, Loss: 1.0292

Epoch 3, Batch 1400, Loss: 1.0504

Epoch 3, Batch 1500, Loss: 1.0425

Epoch 4, Batch 100, Loss: 0.9508

Epoch 4, Batch 200, Loss: 0.9465

Epoch 4, Batch 300, Loss: 0.9708

Epoch 4, Batch 400, Loss: 0.9716

Epoch 4, Batch 500, Loss: 0.9791

Epoch 4, Batch 600, Loss: 0.9554

Epoch 4, Batch 700, Loss: 0.9052

Epoch 4, Batch 800, Loss: 0.9513

Epoch 4, Batch 900, Loss: 0.9679

Epoch 4, Batch 1000, Loss: 0.9559

Epoch 4, Batch 1100, Loss: 0.9495

Epoch 4, Batch 1200, Loss: 0.9179

Epoch 4, Batch 1300, Loss: 0.9553

Epoch 4, Batch 1400, Loss: 0.9579

Epoch 4, Batch 1500, Loss: 0.9295

Epoch 5, Batch 100, Loss: 0.8524

Epoch 5, Batch 200, Loss: 0.8716

Epoch 5, Batch 300, Loss: 0.8879

Epoch 5, Batch 400, Loss: 0.8923

Epoch 5, Batch 500, Loss: 0.8495

Epoch 5, Batch 600, Loss: 0.8430

Epoch 5, Batch 700, Loss: 0.8794

Epoch 5, Batch 800, Loss: 0.8684

Epoch 5, Batch 900, Loss: 0.8922

Epoch 5, Batch 1000, Loss: 0.8721

Epoch 5, Batch 1100, Loss: 0.8792

Epoch 5, Batch 1200, Loss: 0.8439

Epoch 5, Batch 1300, Loss: 0.8792

Epoch 5, Batch 1400, Loss: 0.8805

Epoch 5, Batch 1500, Loss: 0.8825

Epoch 6, Batch 100, Loss: 0.7933

Epoch 6, Batch 200, Loss: 0.8045

Epoch 6, Batch 300, Loss: 0.7912

Epoch 6, Batch 400, Loss: 0.8177

Epoch 6, Batch 500, Loss: 0.7806

Epoch 6, Batch 600, Loss: 0.7857

Epoch 6, Batch 700, Loss: 0.7832

Epoch 6, Batch 800, Loss: 0.8015

Epoch 6, Batch 900, Loss: 0.7819

Epoch 6, Batch 1000, Loss: 0.8249

Epoch 6, Batch 1100, Loss: 0.8002

Epoch 6, Batch 1200, Loss: 0.8297

Epoch 6, Batch 1300, Loss: 0.8108

Epoch 6, Batch 1400, Loss: 0.7939

Epoch 6, Batch 1500, Loss: 0.8641

Epoch 7, Batch 100, Loss: 0.7501

Epoch 7, Batch 200, Loss: 0.7387

Epoch 7, Batch 300, Loss: 0.7316

Epoch 7, Batch 400, Loss: 0.7531

Epoch 7, Batch 500, Loss: 0.7278

Epoch 7, Batch 600, Loss: 0.7850

Epoch 7, Batch 700, Loss: 0.7332

Epoch 7, Batch 800, Loss: 0.7680

Epoch 7, Batch 900, Loss: 0.7306

Epoch 7, Batch 1000, Loss: 0.7396

Epoch 7, Batch 1100, Loss: 0.7488

Epoch 7, Batch 1200, Loss: 0.7181

Epoch 7, Batch 1300, Loss: 0.7554

Epoch 7, Batch 1400, Loss: 0.7397

Epoch 7, Batch 1500, Loss: 0.7846

Epoch 8, Batch 100, Loss: 0.7333

Epoch 8, Batch 200, Loss: 0.6850

Epoch 8, Batch 300, Loss: 0.6613

Epoch 8, Batch 400, Loss: 0.6900

Epoch 8, Batch 500, Loss: 0.6786

Epoch 8, Batch 600, Loss: 0.7015

Epoch 8, Batch 700, Loss: 0.6880

Epoch 8, Batch 800, Loss: 0.7421

Epoch 8, Batch 900, Loss: 0.6941

Epoch 8, Batch 1000, Loss: 0.7093

Epoch 8, Batch 1100, Loss: 0.6958

Epoch 8, Batch 1200, Loss: 0.6910

Epoch 8, Batch 1300, Loss: 0.7353

Epoch 8, Batch 1400, Loss: 0.6959

Epoch 8, Batch 1500, Loss: 0.7144

Epoch 9, Batch 100, Loss: 0.6320

Epoch 9, Batch 200, Loss: 0.6457

Epoch 9, Batch 300, Loss: 0.6711

Epoch 9, Batch 400, Loss: 0.6538

Epoch 9, Batch 500, Loss: 0.6549

Epoch 9, Batch 600, Loss: 0.6692

Epoch 9, Batch 700, Loss: 0.6479

Epoch 9, Batch 800, Loss: 0.6704

Epoch 9, Batch 900, Loss: 0.6613

Epoch 9, Batch 1000, Loss: 0.6563

Epoch 9, Batch 1100, Loss: 0.6720

Epoch 9, Batch 1200, Loss: 0.6402

Epoch 9, Batch 1300, Loss: 0.6596

Epoch 9, Batch 1400, Loss: 0.6970

Epoch 9, Batch 1500, Loss: 0.6844

Epoch 10, Batch 100, Loss: 0.5926

Epoch 10, Batch 200, Loss: 0.6073

Epoch 10, Batch 300, Loss: 0.5925

Epoch 10, Batch 400, Loss: 0.5942

Epoch 10, Batch 500, Loss: 0.6056

Epoch 10, Batch 600, Loss: 0.6177

Epoch 10, Batch 700, Loss: 0.6513

Epoch 10, Batch 800, Loss: 0.6210

Epoch 10, Batch 900, Loss: 0.6399

Epoch 10, Batch 1000, Loss: 0.6473

Epoch 10, Batch 1100, Loss: 0.6256

Epoch 10, Batch 1200, Loss: 0.6020

Epoch 10, Batch 1300, Loss: 0.6528

Epoch 10, Batch 1400, Loss: 0.6337

Epoch 10, Batch 1500, Loss: 0.6203

Finished Training

Accuracy on test set: 73.72%

**Training Configuration:**

**Learning Rate: 0.001**

**Batch Size: 128**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 2.0036

Epoch 1, Batch 200, Loss: 1.6468

Epoch 1, Batch 300, Loss: 1.5207

Epoch 2, Batch 100, Loss: 1.3968

Epoch 2, Batch 200, Loss: 1.3068

Epoch 2, Batch 300, Loss: 1.2843

Epoch 3, Batch 100, Loss: 1.1881

Epoch 3, Batch 200, Loss: 1.1728

Epoch 3, Batch 300, Loss: 1.1244

Epoch 4, Batch 100, Loss: 1.0724

Epoch 4, Batch 200, Loss: 1.0307

Epoch 4, Batch 300, Loss: 1.0119

Epoch 5, Batch 100, Loss: 0.9569

Epoch 5, Batch 200, Loss: 0.9519

Epoch 5, Batch 300, Loss: 0.9265

Epoch 6, Batch 100, Loss: 0.8882

Epoch 6, Batch 200, Loss: 0.8684

Epoch 6, Batch 300, Loss: 0.8642

Epoch 7, Batch 100, Loss: 0.8002

Epoch 7, Batch 200, Loss: 0.8166

Epoch 7, Batch 300, Loss: 0.8097

Epoch 8, Batch 100, Loss: 0.7447

Epoch 8, Batch 200, Loss: 0.7563

Epoch 8, Batch 300, Loss: 0.7793

Epoch 9, Batch 100, Loss: 0.6891

Epoch 9, Batch 200, Loss: 0.6998

Epoch 9, Batch 300, Loss: 0.7088

Epoch 10, Batch 100, Loss: 0.6538

Epoch 10, Batch 200, Loss: 0.6664

Epoch 10, Batch 300, Loss: 0.6535

Finished Training

Accuracy on test set: 74.71%

**Training Configuration:**

**Learning Rate: 0.001**

**Batch Size: 256**

**Optimizer: adam**

**Number of Epochs: 10**

Epoch 1, Batch 100, Loss: 1.9355

Epoch 2, Batch 100, Loss: 1.4673

Epoch 3, Batch 100, Loss: 1.3054

Epoch 4, Batch 100, Loss: 1.1680

Epoch 5, Batch 100, Loss: 1.0819

Epoch 6, Batch 100, Loss: 0.9949

Epoch 7, Batch 100, Loss: 0.9285

Epoch 8, Batch 100, Loss: 0.8777

Epoch 9, Batch 100, Loss: 0.8172

Epoch 10, Batch 100, Loss: 0.7733

Finished Training

Accuracy on test set: 72.88%